# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16jJ4ljjXXROwFePnQgMGEevxDx1NeVA_
"""

import pandas as pd
import numpy as np
import re
import joblib
import math
import tldextract
from urllib.parse import urlparse
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Enhanced feature extraction function
def extract_features(url):
    try:
        # Parse URL components
        parsed_url = urlparse(url)
        extracted = tldextract.extract(url)
        domain = extracted.domain
        suffix = extracted.suffix
        subdomain = extracted.subdomain
        full_domain = f"{domain}.{suffix}"

        # Basic URL features
        url_length = len(url)
        domain_length = len(full_domain)
        has_https = int(parsed_url.scheme == 'https')
        num_dots = url.count('.')
        num_hyphens = url.count('-')
        num_underscores = url.count('_')
        num_slashes = url.count('/')
        num_at_signs = url.count('@')
        num_ampersands = url.count('&')
        num_question_marks = url.count('?')
        num_equals = url.count('=')
        num_digits = sum(c.isdigit() for c in url)
        num_digits_domain = sum(c.isdigit() for c in full_domain)

        # Path features
        has_path = int(len(parsed_url.path) > 1)  # > 1 to exclude just "/"
        path_length = len(parsed_url.path)
        num_path_segments = len([x for x in parsed_url.path.split('/') if x])

        # Query parameters
        has_query = int(len(parsed_url.query) > 0)
        num_query_params = len(parsed_url.query.split('&')) if has_query else 0

        # Domain specific features
        has_subdomain = int(len(subdomain) > 0)
        subdomain_length = len(subdomain) if has_subdomain else 0
        tld_length = len(suffix)

        # Suspicious patterns
        ip_pattern = re.compile(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$')
        is_ip = int(bool(ip_pattern.match(domain)))

        has_port = int('@' in parsed_url.netloc or ':' in parsed_url.netloc)

        suspicious_tlds = ['zip', 'review', 'country', 'kim', 'cricket', 'science', 'work', 'party', 'gq', 'link']
        has_suspicious_tld = int(suffix in suspicious_tlds)

        phishing_terms = ['secure', 'account', 'webscr', 'login', 'ebayisapi', 'signin', 'banking',
                          'confirm', 'update', 'verify', 'paypal', 'password', 'credential']
        contains_phishing_terms = sum(1 for term in phishing_terms if term in url.lower())

        # Shannon entropy of domain - measures randomness which is common in phishing domains
        domain_entropy = 0
        if domain:
            char_counts = {}
            for char in domain:
                if char in char_counts:
                    char_counts[char] += 1
                else:
                    char_counts[char] = 1

            domain_entropy = 0
            for count in char_counts.values():
                freq = count / len(domain)
                domain_entropy -= freq * math.log2(freq)

        # Return all features as a list
        features = [
            url_length, domain_length, has_https, num_dots, num_hyphens, num_underscores,
            num_slashes, num_at_signs, num_ampersands, num_question_marks, num_equals,
            num_digits, num_digits_domain, has_path, path_length, num_path_segments,
            has_query, num_query_params, has_subdomain, subdomain_length, tld_length,
            is_ip, has_port, has_suspicious_tld, contains_phishing_terms, domain_entropy
        ]

        return features

    except Exception as e:
        # Return default values in case of parsing errors
        print(f"Error extracting features from {url}: {e}")
        return [0] * 26  # Return zeros for all features

# Generate more realistic training data - this is for demonstration
# In a real scenario, you would use a large dataset of real phishing and legitimate URLs
def generate_training_data(num_samples=5000):
    # Legitimate domains and patterns
    legitimate_domains = ['google.com', 'amazon.com', 'microsoft.com', 'apple.com', 'facebook.com',
                         'youtube.com', 'instagram.com', 'twitter.com', 'linkedin.com', 'github.com',
                         'stackoverflow.com', 'reddit.com', 'wikipedia.org', 'yahoo.com', 'netflix.com']

    legitimate_patterns = [
        'https://{domain}',
        'https://www.{domain}',
        'https://{domain}/search?q=term',
        'https://{domain}/products/{id}',
        'https://shop.{domain}/item/{id}',
        'https://{domain}/profile/{username}',
        'https://{domain}/about',
        'https://{domain}/contact',
        'https://blog.{domain}/post/{id}',
        'https://support.{domain}/help/{id}'
    ]

    # Phishing patterns
    phishing_domains = [
        f"{legitimate.replace('.com', '')}-login.com" for legitimate in legitimate_domains
    ] + [
        f"secure-{legitimate}" for legitimate in legitimate_domains
    ] + [
        f"{legitimate.split('.')[0]}account.info" for legitimate in legitimate_domains
    ] + [
        f"verify-{legitimate.split('.')[0]}.net" for legitimate in legitimate_domains
    ] + [
        f"{legitimate.split('.')[0]}.{legitimate.split('.')[0]}.com" for legitimate in legitimate_domains
    ]

    phishing_patterns = [
        'http://{domain}/login',
        'http://{domain}/verify-account',
        'http://{domain}/secure-login.php?user={username}',
        'http://{domain}/confirm-identity',
        'http://account-verification.{domain}/login',
        'http://login.{domain}.phishing.com/verify',
        'http://{domain}/reset-password?token=123&email={email}',
        'http://www.{domain}.tk/account/verify',
        'http://secure-{domain}/authenticate',
        'http://{ip}/~user/{domain}/'
    ]

    urls = []
    labels = []

    # Generate legitimate URLs
    for _ in range(num_samples // 2):
        domain = np.random.choice(legitimate_domains)
        pattern = np.random.choice(legitimate_patterns)
        url = pattern.format(
            domain=domain,
            id=np.random.randint(1000, 9999),
            username=f"user{np.random.randint(100, 999)}",
        )
        urls.append(url)
        labels.append(0)  # 0 for legitimate

    # Generate phishing URLs
    for _ in range(num_samples // 2):
        domain = np.random.choice(phishing_domains)
        pattern = np.random.choice(phishing_patterns)
        url = pattern.format(
            domain=domain,
            username=f"user{np.random.randint(100, 999)}",
            email=f"user{np.random.randint(100, 999)}@gmail.com",
            ip=f"{np.random.randint(1, 255)}.{np.random.randint(1, 255)}.{np.random.randint(1, 255)}.{np.random.randint(1, 255)}"
        )
        urls.append(url)
        labels.append(1)  # 1 for phishing

    return pd.DataFrame({'url': urls, 'label': labels})

if __name__ == "__main__":
    print("Generating training data...")
    df = generate_training_data(5000)

    print("Extracting features...")
    # Extract features for all URLs
    X = []
    for url in df['url']:
        X.append(extract_features(url))

    y = df['label'].values

    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    print("Training the model...")
    # Train an ensemble model
    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
    model.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"Model accuracy: {accuracy * 100:.2f}%")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    # Feature importance
    feature_names = [
        'url_length', 'domain_length', 'has_https', 'num_dots', 'num_hyphens', 'num_underscores',
        'num_slashes', 'num_at_signs', 'num_ampersands', 'num_question_marks', 'num_equals',
        'num_digits', 'num_digits_domain', 'has_path', 'path_length', 'num_path_segments',
        'has_query', 'num_query_params', 'has_subdomain', 'subdomain_length', 'tld_length',
        'is_ip', 'has_port', 'has_suspicious_tld', 'contains_phishing_terms', 'domain_entropy'
    ]

    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]

    print("\nFeature Ranking:")
    for i, idx in enumerate(indices[:10]):
        print(f"{i+1}. {feature_names[idx]} ({importances[idx]:.4f})")

    # Save the model
    print("Saving the model...")
    joblib.dump(model, 'pishing_model.pkl')
    print("Model saved as 'phishing_model.pkl'")