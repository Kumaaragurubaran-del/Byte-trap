# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J_WcdmP6xWmXHWOjN1JiYmAmwv34CCyx
"""

import streamlit as st
import joblib
import math
import re
import tldextract
from urllib.parse import urlparse
import pandas as pd

# Load the trained model
@st.cache_resource
def load_model():
    return joblib.load('pishing_model.pkl')

# Feature extraction function (same as in train_model.py)
def extract_features(url):
    try:
        # Parse URL components
        parsed_url = urlparse(url)
        extracted = tldextract.extract(url)
        domain = extracted.domain
        suffix = extracted.suffix
        subdomain = extracted.subdomain
        full_domain = f"{domain}.{suffix}"

        # Basic URL features
        url_length = len(url)
        domain_length = len(full_domain)
        has_https = int(parsed_url.scheme == 'https')
        num_dots = url.count('.')
        num_hyphens = url.count('-')
        num_underscores = url.count('_')
        num_slashes = url.count('/')
        num_at_signs = url.count('@')
        num_ampersands = url.count('&')
        num_question_marks = url.count('?')
        num_equals = url.count('=')
        num_digits = sum(c.isdigit() for c in url)
        num_digits_domain = sum(c.isdigit() for c in full_domain)

        # Path features
        has_path = int(len(parsed_url.path) > 1)  # > 1 to exclude just "/"
        path_length = len(parsed_url.path)
        num_path_segments = len([x for x in parsed_url.path.split('/') if x])

        # Query parameters
        has_query = int(len(parsed_url.query) > 0)
        num_query_params = len(parsed_url.query.split('&')) if has_query else 0

        # Domain specific features
        has_subdomain = int(len(subdomain) > 0)
        subdomain_length = len(subdomain) if has_subdomain else 0
        tld_length = len(suffix)

        # Suspicious patterns
        ip_pattern = re.compile(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$')
        is_ip = int(bool(ip_pattern.match(domain)))

        has_port = int('@' in parsed_url.netloc or ':' in parsed_url.netloc)

        suspicious_tlds = ['zip', 'review', 'country', 'kim', 'cricket', 'science', 'work', 'party', 'gq', 'link']
        has_suspicious_tld = int(suffix in suspicious_tlds)

        phishing_terms = ['secure', 'account', 'webscr', 'login', 'ebayisapi', 'signin', 'banking',
                          'confirm', 'update', 'verify', 'paypal', 'password', 'credential']
        contains_phishing_terms = sum(1 for term in phishing_terms if term in url.lower())

        # Shannon entropy of domain - measures randomness which is common in phishing domains
        domain_entropy = 0
        if domain:
            char_counts = {}
            for char in domain:
                if char in char_counts:
                    char_counts[char] += 1
                else:
                    char_counts[char] = 1

            domain_entropy = 0
            for count in char_counts.values():
                freq = count / len(domain)
                domain_entropy -= freq * math.log2(freq)

        # Return all features as a list
        features = [
            url_length, domain_length, has_https, num_dots, num_hyphens, num_underscores,
            num_slashes, num_at_signs, num_ampersands, num_question_marks, num_equals,
            num_digits, num_digits_domain, has_path, path_length, num_path_segments,
            has_query, num_query_params, has_subdomain, subdomain_length, tld_length,
            is_ip, has_port, has_suspicious_tld, contains_phishing_terms, domain_entropy
        ]

        return features

    except Exception as e:
        st.error(f"Error extracting features from URL: {e}")
        return [0] * 26  # Return zeros for all features

# Function to analyze URL and explain the decision
def analyze_url(url, model):
    # Extract features
    features = extract_features(url)

    # Create DataFrame for explanation
    feature_names = [
        'URL Length', 'Domain Length', 'HTTPS', 'Number of Dots', 'Number of Hyphens', 'Number of Underscores',
        'Number of Slashes', 'Number of @ Signs', 'Number of Ampersands', 'Number of Question Marks', 'Number of Equals',
        'Number of Digits', 'Number of Digits in Domain', 'Has Path', 'Path Length', 'Number of Path Segments',
        'Has Query', 'Number of Query Parameters', 'Has Subdomain', 'Subdomain Length', 'TLD Length',
        'Is IP Address', 'Has Port', 'Has Suspicious TLD', 'Contains Phishing Terms', 'Domain Entropy'
    ]

    feature_df = pd.DataFrame({
        'Feature': feature_names,
        'Value': features
    })

    # Make prediction
    prediction = model.predict([features])[0]
    probability = model.predict_proba([features])[0]

    # Get feature importance for this prediction
    if hasattr(model, 'feature_importances_'):
        feature_df['Importance'] = model.feature_importances_
        feature_df = feature_df.sort_values('Importance', ascending=False)

    return prediction, probability, feature_df

def main():
    st.set_page_config(
        page_title="Advanced Phishing URL Detector",
        page_icon="üõ°Ô∏è",
        layout="wide"
    )

    st.title("üõ°Ô∏è Advanced Phishing URL Detector")
    st.subheader("Check if a URL is legitimate or potentially malicious")

    # Load model
    try:
        model = load_model()
    except Exception as e:
        st.error(f"Error loading model: {e}")
        st.info("Please run the training script first to generate the model.")
        return

    # URL input
    url = st.text_input("Enter a URL to analyze:", "https://example.com")

    if url:
        with st.spinner("Analyzing URL..."):
            # Analyze the URL
            prediction, probability, feature_df = analyze_url(url, model)

            # Display the result with probability
            col1, col2 = st.columns([1, 2])

            with col1:
                if prediction == 0:
                    st.success("‚úÖ SAFE URL")
                    safe_prob = probability[0] * 100
                    st.metric("Probability of being safe", f"{safe_prob:.1f}%")
                    result_color = "green"
                else:
                    st.error("‚ö†Ô∏è POTENTIALLY PHISHING")
                    phish_prob = probability[1] * 100
                    st.metric("Probability of being phishing", f"{phish_prob:.1f}%")
                    result_color = "red"

                # Extract and display domain information
                try:
                    extracted = tldextract.extract(url)
                    domain = extracted.domain
                    suffix = extracted.suffix
                    subdomain = extracted.subdomain

                    st.subheader("Domain Information")
                    st.write(f"**Full Domain:** {domain}.{suffix}")
                    if subdomain:
                        st.write(f"**Subdomain:** {subdomain}")
                    st.write(f"**TLD:** {suffix}")

                    # URL structure
                    parsed_url = urlparse(url)
                    st.subheader("URL Structure")
                    st.write(f"**Scheme:** {parsed_url.scheme}")
                    st.write(f"**Path:** {parsed_url.path}")
                    if parsed_url.query:
                        st.write(f"**Query Parameters:** {parsed_url.query}")
                except Exception as e:
                    st.warning(f"Could not parse domain information: {e}")

            with col2:
                # Show top features that influenced the decision
                st.subheader("Features Analysis")
                st.dataframe(
                    feature_df.head(10)[['Feature', 'Value', 'Importance']].style.background_gradient(
                        subset=['Importance'], cmap='Blues'
                    )
                )

                # Warning signs
                suspicious_signs = []
                if feature_df.loc[feature_df['Feature'] == 'Is IP Address', 'Value'].values[0] == 1:
                    suspicious_signs.append("‚ùå URL contains an IP address instead of a domain name")
                if feature_df.loc[feature_df['Feature'] == 'Has Suspicious TLD', 'Value'].values[0] == 1:
                    suspicious_signs.append("‚ùå URL uses a suspicious top-level domain")
                if feature_df.loc[feature_df['Feature'] == 'Contains Phishing Terms', 'Value'].values[0] > 0:
                    suspicious_signs.append(f"‚ùå URL contains {int(feature_df.loc[feature_df['Feature'] == 'Contains Phishing Terms', 'Value'].values[0])} phishing-related terms")
                filtered_values = feature_df.loc[feature_df['Feature'] == 'Has @ Signs', 'Value'].values
                if filtered_values.size > 0 and filtered_values[0] > 0:
                    suspicious_signs.append("‚ùå URL contains @ symbol (often used to obscure the actual destination)")
                if feature_df.loc[feature_df['Feature'] == 'HTTPS', 'Value'].values[0] == 0:
                    suspicious_signs.append("‚ö†Ô∏è URL does not use HTTPS (not secure)")

                if suspicious_signs:
                    st.subheader("Suspicious indicators:")
                    for sign in suspicious_signs:
                        st.markdown(sign)

                # Safety tips
                st.subheader("Safety Tips:")
                st.info("""
                ‚Ä¢ Always check the URL in your browser's address bar before entering credentials
                ‚Ä¢ Legitimate websites use HTTPS encryption
                ‚Ä¢ Be wary of URLs that contain IP addresses instead of domain names
                ‚Ä¢ Check for misspellings or suspicious domains that imitate legitimate websites
                ‚Ä¢ Hover over links to see their actual destination before clicking
                """)

        # Additional UI elements for transparency
        with st.expander("View all extracted features"):
            st.dataframe(feature_df)

    st.markdown("---")

if __name__ == "__main__":
    main()